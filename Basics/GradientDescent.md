# Gradient Descent

#### 경사하강법 (Optimizer)
손실 함수의 기울기(Gradient)를 이용해, 현재 위치에서 가장 가파르게 내려가는 방향(손실을 줄이는 방향)으로 모델의 파라미터(예: 가중치)를 업데이트하는 방법

**경사하강법의 기본 아이디어**
1. 손실 함수의 기울기 계산
2. 기울기가 가리키는 방향으로 파라미터 업데이트
3. 손실이 줄어들도록 반복해서 이 과정 수행

학습률 (learning rate)는 경사하강법에서 파라미터 업데이트 크기를 조절하는 역할을 한다.
![500](https://i.imgur.com/achhG3O.png)

#### 표준 경사하강법 vs 확률적 경사하강법
![](https://i.imgur.com/8Zj1DsF.png)


![500](https://i.imgur.com/0Mtn487.png)

#### **표준 경사하강법** **(Batch Gradient Descent)**
- 전체 데이터 세트를 사용하여 기울기를 계산
- **손실 함수의 기울기**를 구하기 위해 훈련 데이터의 모든 샘플을 고려
- 전체 데이터 세트를 사용하기 때문에 기울기 계산이 정확하지만 계산 비용이 크고 느림
**단점**
- 기울기 계산이 정확하여 최적화 과정이 안정적이지만 학습이 느리며 메모리 소모가 큼
- 큰 데이터 세트에서는 반복마다 전체 데이터를 처리해야 하므로 비효율적

#### **확률적 경사하강법** **(Stochastic Gradient Descent)**
- 하나의 데이터 포인트(또는 소규모 배치)를 사용하여 기울기를 계산
- 각 반복에서 훈련데이터의 개별 샘플만을 사용해 기울기를 계산하고 그에 따라 파라미터 업데이트
- 계산이 빠르고 메모리 사용량이 적음
- local minimum에 갇히는 문제를 완화하고 더 나은 일반화를 유도할 수 있음
**단점**
- 각 업데이트가 무작위 샘플에 기반하므로 기울기 계산이 불안정하여 최적화 과정에서 진동이 발생할 수 있음
- 수렴이 불규칙함

최근 연구에는 SGD의 문제점을 인지하고 각각의 문제점을 개선하는 더 좋은 Optimizer들이 많이 있음
![](https://i.imgur.com/AABnTeG.png)

